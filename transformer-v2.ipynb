{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce61c533-2231-4f2c-8f4a-d5fd077f9905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import cbor2\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "summary = SummaryWriter(\"runs/attention-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ee65534-6984-41ce-8299-bdf67ac2cd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS\n",
    "CONTEXT_LEN = 256\n",
    "BATCH_SIZE = 512\n",
    "EMBED_LEN = 32\n",
    "ADDED_PARAM_LEN = 3 # duration, delay, velocity\n",
    "PARAM_LEN = 1 + ADDED_PARAM_LEN\n",
    "DROPOUT_RATE = 0.2\n",
    "TOKEN_LEN = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aff63cf-a906-4e18-b939-4302c6a44242",
   "metadata": {},
   "outputs": [],
   "source": [
    "track_paths = glob.glob('/home/bob/vmshare/midi/solo-piano/*.tokens')\n",
    "#display(track_paths)\n",
    "tracks = []\n",
    "for path in track_paths:\n",
    "    with open(path, 'rb') as f:\n",
    "        track_data = cbor2.load(f)\n",
    "        if len(track_data) < 100:\n",
    "            continue\n",
    "        tracks.append(track_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2d10b79-a6f0-43e9-9675-2b85c7cd9afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 0.0, 0.0, 0.0], [0, 5.3125, 0.10416666666666667, 0.007874015748031496], [56, 0.19791666666666666, 5.791666666666667, 0.84251968503937], [56, 1.15625, 0.28125, 0.8346456692913385], [32, 0.0, 3.0625, 0.33070866141732286]]\n",
      "72\n"
     ]
    }
   ],
   "source": [
    "print(tracks[5][0:5])\n",
    "print(len(tracks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "049fe1ac-2fda-4709-8948-88139476e1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor([event for track in tracks for event in track])\n",
    "split_n = int(0.9 * len(data))\n",
    "train_data = data[:split_n]\n",
    "eval_data = data[split_n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb7f9cae-8dda-42b4-9bec-451f26b381ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else eval_data\n",
    "    ix = torch.randint(len(data) - CONTEXT_LEN, (BATCH_SIZE,))\n",
    "    x = torch.stack([data[i:i+CONTEXT_LEN] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+CONTEXT_LEN+1] for i in ix])\n",
    "    return x.cuda(), y.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07a08b27-4403-4603-b45c-9275af874383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 256, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_batch('train')[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4f152ee-d522-4fc9-98c3-bfc3e56eadff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding:\n",
    "    def __init__(self, num_embeddings, embeddings_dim) -> None:\n",
    "        self.weight = torch.randn((num_embeddings, embeddings_dim), device=\"cuda\")\n",
    "\n",
    "    def __call__(self, IX):\n",
    "        # print(IX.shape)\n",
    "        ix = IX[:,:,0].int()\n",
    "        added = IX[:,:,1:].view(-1, CONTEXT_LEN, ADDED_PARAM_LEN)\n",
    "        # print(f\"{ix=}\")\n",
    "        emb = self.weight[ix]\n",
    "        self.out = torch.cat((emb, added), dim=2)\n",
    "        # print(f\"{self.out=}\")\n",
    "        # self.out = self.weight[IX]\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight]\n",
    "\n",
    "class Linear:\n",
    "    def __init__( self, fan_in: int, fan_out: int, bias=True, device=\"cuda\"):\n",
    "        self.device = device\n",
    "        # Random initialization with kaiming-like normalization.\n",
    "        self.weight = (\n",
    "            torch.randn((fan_in, fan_out), device=device)\n",
    "            / fan_in**0.5\n",
    "        )\n",
    "        self.bias = torch.zeros(fan_out, device=device) if bias else None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weight\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weight] + ([] if self.bias is None else [self.bias])\n",
    "\n",
    "class BatchNorm1d:\n",
    "    \"\"\"\\\n",
    "    Batch Noemalization: Accelerating Deep Network Training by Reducing Internal\n",
    "    Covariant Shift\n",
    "    https://arxiv.org/abs/1502.03167\n",
    "\n",
    "    Same as torch.nn.BatchNorm1d with:\n",
    "        affine=True # will use beta and gamma\n",
    "        track_running_stats=True\n",
    "        device=whatever\n",
    "        dtype=float\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, epsilon=1e-5, momentum=0.1, device=\"cuda\"):\n",
    "        self.device = device\n",
    "        self.eps = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.training = True\n",
    "        self.gamma = torch.ones(dim, device=device)\n",
    "        self.beta = torch.zeros(dim, device=device)\n",
    "        # buffers, trained with running \"momentum update\" for smoothing\n",
    "        self.running_mean = torch.zeros(dim, device=device)\n",
    "        # tracking variance instead of stddev\n",
    "        self.running_var = torch.ones(dim, device=device)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            if x.ndim == 2:\n",
    "                dim = 0\n",
    "            elif x.ndim == 3:\n",
    "                dim = (0, 1)\n",
    "            xmean = x.mean(0, keepdim=True)\n",
    "            xvar = x.var(0, keepdim=True)\n",
    "        else:\n",
    "            xmean = self.running_mean  # µB from paper\n",
    "            xvar = self.running_var  # σ^2B from paper\n",
    "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
    "        # used for statistic collection, not part of torch\n",
    "        self.out = self.gamma * xhat + self.beta\n",
    "        # update buffers\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = (\n",
    "                    1 - self.momentum\n",
    "                ) * self.running_mean + self.momentum * xmean\n",
    "                self.running_var = (\n",
    "                    1 - self.momentum\n",
    "                ) * self.running_var + self.momentum * xvar\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "\n",
    "class Tanh:\n",
    "    \"\"\"Same as torch.nn.Tanh\"\"\"\n",
    "\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Flatten:\n",
    "    def __call__(self, x):\n",
    "        self.out = x.view(x.shape[0], -1)\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return []\n",
    "\n",
    "class Sequential:\n",
    "    def __init__(self, *layers) -> None:\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        self.out = x\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d97a66e-bdd2-4567-8bf9-31a37b88e134",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(EMBED_LEN, head_size, bias=False)\n",
    "        self.query = nn.Linear(EMBED_LEN, head_size, bias=False)\n",
    "        self.value = nn.Linear(EMBED_LEN, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(DROPOUT_RATE)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(CONTEXT_LEN, CONTEXT_LEN)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        w = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        w = w.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        w = F.softmax(w, dim=-1)\n",
    "        w = self.dropout(w)\n",
    "        v = self.value(x)\n",
    "        out = w @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([SelfAttention(head_size) for _ in range(num_heads)])\n",
    "        self.projection = nn.Linear(num_heads * head_size, EMBED_LEN)\n",
    "        self.dropout = nn.Dropout(DROPOUT_RATE)\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.projection(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embed, n_embed),\n",
    "            nn.Dropout(DROPOUT_RATE),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class SABlock(nn.Module):\n",
    "    def __init__(self, n_embed, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embed + ADDED_PARAM_LEN)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed + ADDED_PARAM_LEN)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = x[:,:,:-ADDED_PARAM_LEN]\n",
    "        added = x[:,:,-ADDED_PARAM_LEN:].view(-1, CONTEXT_LEN, ADDED_PARAM_LEN)\n",
    "        x = embeds + self.sa(self.ln1(embeds))\n",
    "        x = torch.cat((x, added), dim=2)\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4823ed7d-46c4-4d71-88a9-e8400508dbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_hidden_neurons, n_sa_heads=4):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(TOKEN_LEN, EMBED_LEN)\n",
    "        self.position_embedding_table = nn.Embedding(CONTEXT_LEN, EMBED_LEN)\n",
    "        # self.sa_head = SelfAttention(EMBED_LEN)\n",
    "        self.blocks = nn.Sequential(\n",
    "            SABlock(EMBED_LEN, 4),\n",
    "            SABlock(EMBED_LEN, 4),\n",
    "            SABlock(EMBED_LEN, 4),\n",
    "            SABlock(EMBED_LEN, 4),\n",
    "            SABlock(EMBED_LEN, 4),\n",
    "            SABlock(EMBED_LEN, 4),\n",
    "            SABlock(EMBED_LEN, 4),\n",
    "            SABlock(EMBED_LEN, 4),\n",
    "        )\n",
    "        #self.sa_heads = MultiHeadAttention(n_sa_heads, EMBED_LEN // n_sa_heads)\n",
    "        #self.ffwd = FeedForward(EMBED_LEN + ADDED_PARAM_LEN)\n",
    "        self.lm_head = Linear((EMBED_LEN + ADDED_PARAM_LEN), TOKEN_LEN, bias=False)\n",
    "        \n",
    "    def forward(self, x, targets=None):\n",
    "        B, T, C = x.shape\n",
    "        tokens = x[:,:,0].long()\n",
    "        added = x[:,:,1:].view(-1, CONTEXT_LEN, ADDED_PARAM_LEN)\n",
    "        tok_emb = self.token_embedding_table(tokens)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device='cuda'))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = torch.cat((x, added), dim=2)\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            classes = targets[:, :, 0].long()\n",
    "            added_params = targets[:, :, 1:]\n",
    "            label_loss = F.cross_entropy(logits.view(B*T, C), classes.view(B*T))\n",
    "            sq_err = ((added_params[:, :, -ADDED_PARAM_LEN:] - logits[:, :, -ADDED_PARAM_LEN:]) ** 2)\n",
    "            added_param_loss = sq_err.mean(dim=1).mean(dim=0).sum()\n",
    "            loss = label_loss + added_param_loss\n",
    "        return logits, loss, label_loss, added_param_loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        model.eval()\n",
    "        out = []\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx = idx[:, -CONTEXT_LEN:] # truncate context\n",
    "            logits, loss = self(idx)\n",
    "            probs = F.softmax(logits[:, -1, :TOKEN_LEN], dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1) # B, 1\n",
    "            token = next_token.int().item()\n",
    "            added_params = logits[:,-1,-ADDED_PARAM_LEN:].view(1, ADDED_PARAM_LEN)\n",
    "            next_idx = torch.cat((next_token, added_params), dim=-1) # B, 3\n",
    "            out.append([token] + added_params.tolist()[0])\n",
    "            idx = torch.cat((idx, next_idx.view(1, -1, 3)), dim=1)\n",
    "            if next_token.item() == 3:\n",
    "                break\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59797891-dff7-4de0-932c-2771dffae70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(BATCH_SIZE).cuda()\n",
    "summary.add_graph(model, get_batch('train'))\n",
    "global_training_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad5e7acd-d020-4e22-b3cd-5189603fdb05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      0/ 100000: 4.8407\n",
      "    100/ 100000: 4.7694\n",
      "    200/ 100000: 4.7153\n",
      "    300/ 100000: 4.7356\n",
      "    400/ 100000: 4.7919\n",
      "    500/ 100000: 4.6977\n",
      "    600/ 100000: 4.6393\n",
      "    700/ 100000: 4.7098\n",
      "    800/ 100000: 4.6437\n",
      "    900/ 100000: 4.6194\n",
      "   1000/ 100000: 4.6096\n",
      "   1100/ 100000: 4.6061\n",
      "   1200/ 100000: 4.6203\n",
      "   1300/ 100000: 4.5783\n",
      "   1400/ 100000: 4.5780\n",
      "   1500/ 100000: 4.5592\n",
      "   1600/ 100000: 4.5557\n",
      "   1700/ 100000: 4.5577\n",
      "   1800/ 100000: 4.5026\n",
      "   1900/ 100000: 4.5923\n",
      "   2000/ 100000: 4.5990\n",
      "   2100/ 100000: 4.4858\n",
      "   2200/ 100000: 4.5252\n",
      "   2300/ 100000: 4.4919\n",
      "   2400/ 100000: 4.4999\n",
      "   2500/ 100000: 4.4814\n",
      "   2600/ 100000: 4.4800\n",
      "   2700/ 100000: 4.4234\n",
      "   2800/ 100000: 4.4918\n",
      "   2900/ 100000: 4.3921\n",
      "   3000/ 100000: 4.4280\n",
      "   3100/ 100000: 4.4470\n",
      "   3200/ 100000: 4.4089\n",
      "   3300/ 100000: 4.3865\n",
      "   3400/ 100000: 4.3704\n",
      "   3500/ 100000: 4.3718\n",
      "   3600/ 100000: 4.4002\n",
      "   3700/ 100000: 4.3564\n",
      "   3800/ 100000: 4.3796\n",
      "   3900/ 100000: 4.3072\n",
      "   4000/ 100000: 4.3215\n",
      "   4100/ 100000: 4.3492\n",
      "   4200/ 100000: 4.2981\n",
      "   4300/ 100000: 4.3612\n",
      "   4400/ 100000: 4.3371\n",
      "   4500/ 100000: 4.2956\n",
      "   4600/ 100000: 4.2489\n",
      "   4700/ 100000: 4.3341\n",
      "   4800/ 100000: 4.3134\n",
      "   4900/ 100000: 4.2271\n",
      "   5000/ 100000: 4.2580\n",
      "   5100/ 100000: 4.2619\n",
      "   5200/ 100000: 4.2763\n",
      "   5300/ 100000: 4.2308\n",
      "   5400/ 100000: 4.2474\n",
      "   5500/ 100000: 4.2198\n",
      "   5600/ 100000: 4.2395\n",
      "   5700/ 100000: 4.1801\n",
      "   5800/ 100000: 4.2152\n",
      "   5900/ 100000: 4.2512\n",
      "   6000/ 100000: 4.2263\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 21\u001b[0m \u001b[40moptimizer\u001b[49m\u001b[38;5;241;40m.\u001b[39;49m\u001b[40mstep\u001b[49m\u001b[40m(\u001b[49m\u001b[40m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# track\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~/src/midi-tokenizer/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    371\u001b[0m             )\n\u001b[1;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[40mfunc\u001b[49m\u001b[40m(\u001b[49m\u001b[38;5;241;40m*\u001b[39;49m\u001b[40margs\u001b[49m\u001b[40m,\u001b[49m\u001b[40m \u001b[49m\u001b[38;5;241;40m*\u001b[39;49m\u001b[38;5;241;40m*\u001b[39;49m\u001b[40mkwargs\u001b[49m\u001b[40m)\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~/src/midi-tokenizer/.venv/lib/python3.11/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[40mfunc\u001b[49m\u001b[40m(\u001b[49m\u001b[38;5;28;40mself\u001b[39;49m\u001b[40m,\u001b[49m\u001b[40m \u001b[49m\u001b[38;5;241;40m*\u001b[39;49m\u001b[40margs\u001b[49m\u001b[40m,\u001b[49m\u001b[40m \u001b[49m\u001b[38;5;241;40m*\u001b[39;49m\u001b[38;5;241;40m*\u001b[39;49m\u001b[40mkwargs\u001b[49m\u001b[40m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~/src/midi-tokenizer/.venv/lib/python3.11/site-packages/torch/optim/adamw.py:184\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    171\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    174\u001b[0m         group,\n\u001b[0;32m    175\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    181\u001b[0m         state_steps,\n\u001b[0;32m    182\u001b[0m     )\n\u001b[1;32m--> 184\u001b[0m     \u001b[40madamw\u001b[49m\u001b[40m(\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[40m        \u001b[49m\u001b[40mparams_with_grad\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[40m        \u001b[49m\u001b[40mgrads\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[40m        \u001b[49m\u001b[40mexp_avgs\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    188\u001b[0m \u001b[40m        \u001b[49m\u001b[40mexp_avg_sqs\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[40m        \u001b[49m\u001b[40mmax_exp_avg_sqs\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[40m        \u001b[49m\u001b[40mstate_steps\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[40m        \u001b[49m\u001b[40mamsgrad\u001b[49m\u001b[38;5;241;40m=\u001b[39;49m\u001b[40mamsgrad\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[40m        \u001b[49m\u001b[40mbeta1\u001b[49m\u001b[38;5;241;40m=\u001b[39;49m\u001b[40mbeta1\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[40m        \u001b[49m\u001b[40mbeta2\u001b[49m\u001b[38;5;241;40m=\u001b[39;49m\u001b[40mbeta2\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[40m        \u001b[49m\u001b[40mlr\u001b[49m\u001b[38;5;241;40m=\u001b[39;49m\u001b[40mgroup\u001b[49m\u001b[40m[\u001b[49m\u001b[38;5;124;40m\"\u001b[39;49m\u001b[38;5;124;40mlr\u001b[39;49m\u001b[38;5;124;40m\"\u001b[39;49m\u001b[40m]\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[40m        \u001b[49m\u001b[40mweight_decay\u001b[49m\u001b[38;5;241;40m=\u001b[39;49m\u001b[40mgroup\u001b[49m\u001b[40m[\u001b[49m\u001b[38;5;124;40m\"\u001b[39;49m\u001b[38;5;124;40mweight_decay\u001b[39;49m\u001b[38;5;124;40m\"\u001b[39;49m\u001b[40m]\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[40m        \u001b[49m\u001b[40meps\u001b[49m\u001b[38;5;241;40m=\u001b[39;49m\u001b[40mgroup\u001b[49m\u001b[40m[\u001b[49m\u001b[38;5;124;40m\"\u001b[39;49m\u001b[38;5;124;40meps\u001b[39;49m\u001b[38;5;124;40m\"\u001b[39;49m\u001b[40m]\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[40m        \u001b[49m\u001b[40mmaximize\u001b[49m\u001b[38;5;241;40m=\u001b[39;49m\u001b[40mgroup\u001b[49m\u001b[40m[\u001b[49m\u001b[38;5;124;40m\"\u001b[39;49m\u001b[38;5;124;40mmaximize\u001b[39;49m\u001b[38;5;124;40m\"\u001b[39;49m\u001b[40m]\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[40m        \u001b[49m\u001b[40mforeach\u001b[49m\u001b[38;5;241;40m=\u001b[39;49m\u001b[40mgroup\u001b[49m\u001b[40m[\u001b[49m\u001b[38;5;124;40m\"\u001b[39;49m\u001b[38;5;124;40mforeach\u001b[39;49m\u001b[38;5;124;40m\"\u001b[39;49m\u001b[40m]\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[40m        \u001b[49m\u001b[40mcapturable\u001b[49m\u001b[38;5;241;40m=\u001b[39;49m\u001b[40mgroup\u001b[49m\u001b[40m[\u001b[49m\u001b[38;5;124;40m\"\u001b[39;49m\u001b[38;5;124;40mcapturable\u001b[39;49m\u001b[38;5;124;40m\"\u001b[39;49m\u001b[40m]\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[40m        \u001b[49m\u001b[40mdifferentiable\u001b[49m\u001b[38;5;241;40m=\u001b[39;49m\u001b[40mgroup\u001b[49m\u001b[40m[\u001b[49m\u001b[38;5;124;40m\"\u001b[39;49m\u001b[38;5;124;40mdifferentiable\u001b[39;49m\u001b[38;5;124;40m\"\u001b[39;49m\u001b[40m]\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[40m        \u001b[49m\u001b[40mfused\u001b[49m\u001b[38;5;241;40m=\u001b[39;49m\u001b[40mgroup\u001b[49m\u001b[40m[\u001b[49m\u001b[38;5;124;40m\"\u001b[39;49m\u001b[38;5;124;40mfused\u001b[39;49m\u001b[38;5;124;40m\"\u001b[39;49m\u001b[40m]\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[40m        \u001b[49m\u001b[40mgrad_scale\u001b[49m\u001b[38;5;241;40m=\u001b[39;49m\u001b[38;5;28;40mgetattr\u001b[39;49m\u001b[40m(\u001b[49m\u001b[38;5;28;40mself\u001b[39;49m\u001b[40m,\u001b[49m\u001b[40m \u001b[49m\u001b[38;5;124;40m\"\u001b[39;49m\u001b[38;5;124;40mgrad_scale\u001b[39;49m\u001b[38;5;124;40m\"\u001b[39;49m\u001b[40m,\u001b[49m\u001b[40m \u001b[49m\u001b[38;5;28;40;01mNone\u001b[39;49;00m\u001b[40m)\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[40m        \u001b[49m\u001b[40mfound_inf\u001b[49m\u001b[38;5;241;40m=\u001b[39;49m\u001b[38;5;28;40mgetattr\u001b[39;49m\u001b[40m(\u001b[49m\u001b[38;5;28;40mself\u001b[39;49m\u001b[40m,\u001b[49m\u001b[40m \u001b[49m\u001b[38;5;124;40m\"\u001b[39;49m\u001b[38;5;124;40mfound_inf\u001b[39;49m\u001b[38;5;124;40m\"\u001b[39;49m\u001b[40m,\u001b[49m\u001b[40m \u001b[49m\u001b[38;5;28;40;01mNone\u001b[39;49;00m\u001b[40m)\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[40m    \u001b[49m\u001b[40m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~/src/midi-tokenizer/.venv/lib/python3.11/site-packages/torch/optim/adamw.py:335\u001b[0m, in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    333\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[1;32m--> 335\u001b[0m \u001b[40mfunc\u001b[49m\u001b[40m(\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[40m    \u001b[49m\u001b[40mparams\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[40m    \u001b[49m\u001b[40mgrads\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[40m    \u001b[49m\u001b[40mexp_avgs\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[40m    \u001b[49m\u001b[40mexp_avg_sqs\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[40m    \u001b[49m\u001b[40mmax_exp_avg_sqs\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[40m    \u001b[49m\u001b[40mstate_steps\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[40m    \u001b[49m\u001b[40mamsgrad\u001b[49m\u001b[38;5;241;40m=\u001b[39;49m\u001b[40mamsgrad\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[40m    \u001b[49m\u001b[40mbeta1\u001b[49m\u001b[38;5;241;40m=\u001b[39;49m\u001b[40mbeta1\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[40m    \u001b[49m\u001b[40mbeta2\u001b[49m\u001b[38;5;241;40m=\u001b[39;49m\u001b[40mbeta2\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[40m    \u001b[49m\u001b[40mlr\u001b[49m\u001b[38;5;241;40m=\u001b[39;49m\u001b[40mlr\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[40m    \u001b[49m\u001b[40mweight_decay\u001b[49m\u001b[38;5;241;40m=\u001b[39;49m\u001b[40mweight_decay\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[40m    \u001b[49m\u001b[40meps\u001b[49m\u001b[38;5;241;40m=\u001b[39;49m\u001b[40meps\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[40m    \u001b[49m\u001b[40mmaximize\u001b[49m\u001b[38;5;241;40m=\u001b[39;49m\u001b[40mmaximize\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[40m    \u001b[49m\u001b[40mcapturable\u001b[49m\u001b[38;5;241;40m=\u001b[39;49m\u001b[40mcapturable\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[40m    \u001b[49m\u001b[40mdifferentiable\u001b[49m\u001b[38;5;241;40m=\u001b[39;49m\u001b[40mdifferentiable\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[40m    \u001b[49m\u001b[40mgrad_scale\u001b[49m\u001b[38;5;241;40m=\u001b[39;49m\u001b[40mgrad_scale\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[40m    \u001b[49m\u001b[40mfound_inf\u001b[49m\u001b[38;5;241;40m=\u001b[39;49m\u001b[40mfound_inf\u001b[49m\u001b[40m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[40m\u001b[49m\u001b[40m)\u001b[49m\n",
      "File \u001b[1;32m~/src/midi-tokenizer/.venv/lib/python3.11/site-packages/torch/optim/adamw.py:543\u001b[0m, in \u001b[0;36m_multi_tensor_adamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    540\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_lerp_(device_exp_avgs, device_grads, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m    542\u001b[0m torch\u001b[38;5;241m.\u001b[39m_foreach_mul_(device_exp_avg_sqs, beta2)\n\u001b[1;32m--> 543\u001b[0m \u001b[40mtorch\u001b[49m\u001b[38;5;241;40m.\u001b[39;49m\u001b[40m_foreach_addcmul_\u001b[49m\u001b[40m(\u001b[49m\u001b[40mdevice_exp_avg_sqs\u001b[49m\u001b[40m,\u001b[49m\u001b[40m \u001b[49m\u001b[40mdevice_grads\u001b[49m\u001b[40m,\u001b[49m\u001b[40m \u001b[49m\u001b[40mdevice_grads\u001b[49m\u001b[40m,\u001b[49m\u001b[40m \u001b[49m\u001b[38;5;241;40m1\u001b[39;49m\u001b[40m \u001b[49m\u001b[38;5;241;40m-\u001b[39;49m\u001b[40m \u001b[49m\u001b[40mbeta2\u001b[49m\u001b[40m)\u001b[49m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;66;03m# Delete the local intermediate since it won't be used anymore to save on peak memory\u001b[39;00m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m device_grads\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lossi = []\n",
    "model.train()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "max_steps = 100_000\n",
    "# print(sum(1 for p in model.parameters()))\n",
    "# assert Xtrain.device == 'cuda:0', Xtrain.device\n",
    "for i in range(max_steps+1):\n",
    "    global_training_steps += 1\n",
    "    Xb, Yb = get_batch('train')\n",
    "    # forward\n",
    "    #if logits.isnan().any():\n",
    "    #    print(\"NAN ALERT\")\n",
    "    #    break\n",
    "    logits, loss, label_loss, added_param_loss = model(Xb, Yb)\n",
    "    if i % 10 == 0:\n",
    "        summary.add_scalar('loss', loss.item(), global_training_steps)\n",
    "        summary.add_scalar('label_loss', label_loss.item(), global_training_steps)\n",
    "        summary.add_scalar('added_param_loss', added_param_loss.item(), global_training_steps)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # track\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f\"{i:7d}/{max_steps:7d}: {loss.item():.4f}\")\n",
    "    lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36f1bbef-28fb-4770-871d-e89eb09e9ef8",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[-1, 1000]' is invalid for input of size 271",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m16\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# instead, average across 1_000 points\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[40mtorch\u001b[49m\u001b[38;5;241;40m.\u001b[39;49m\u001b[40mtensor\u001b[49m\u001b[40m(\u001b[49m\u001b[40mlossi\u001b[49m\u001b[40m)\u001b[49m\u001b[40m[\u001b[49m\u001b[40m:\u001b[49m\u001b[38;5;241;40m-\u001b[39;49m\u001b[38;5;241;40m1\u001b[39;49m\u001b[40m]\u001b[49m\u001b[38;5;241;40m.\u001b[39;49m\u001b[40mview\u001b[49m\u001b[40m(\u001b[49m\u001b[38;5;241;40m-\u001b[39;49m\u001b[38;5;241;40m1\u001b[39;49m\u001b[40m,\u001b[49m\u001b[40m \u001b[49m\u001b[38;5;241;40m1000\u001b[39;49m\u001b[40m)\u001b[49m\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[-1, 1000]' is invalid for input of size 271"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "# instead, average across 1_000 points\n",
    "plt.plot(torch.tensor(lossi)[:-1].view(-1, 1000).mean(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc6531a8-93c7-4044-9415-30a04ee4f954",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 2]' is invalid for input of size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      2\u001b[0m     gen_ctx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m]] \u001b[38;5;241m*\u001b[39m (CONTEXT_LEN \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m [[\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m]]], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m     generated \u001b[38;5;241m=\u001b[39m \u001b[40mmodel\u001b[49m\u001b[38;5;241;40m.\u001b[39;49m\u001b[40mgenerate\u001b[49m\u001b[40m(\u001b[49m\u001b[40mgen_ctx\u001b[49m\u001b[40m,\u001b[49m\u001b[40m \u001b[49m\u001b[40mmax_new_tokens\u001b[49m\u001b[38;5;241;40m=\u001b[39;49m\u001b[38;5;241;40m1_000\u001b[39;49m\u001b[40m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      5\u001b[0m         cbor2\u001b[38;5;241m.\u001b[39mdump(generated, f)\n",
      "Cell \u001b[1;32mIn[10], line 48\u001b[0m, in \u001b[0;36mModel.generate\u001b[1;34m(self, idx, max_new_tokens)\u001b[0m\n\u001b[0;32m     46\u001b[0m next_token \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(probs, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# B, 1\u001b[39;00m\n\u001b[0;32m     47\u001b[0m token \u001b[38;5;241m=\u001b[39m next_token\u001b[38;5;241m.\u001b[39mint()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m---> 48\u001b[0m added_params \u001b[38;5;241m=\u001b[39m \u001b[40mlogits\u001b[49m\u001b[40m[\u001b[49m\u001b[40m:\u001b[49m\u001b[40m,\u001b[49m\u001b[38;5;241;40m-\u001b[39;49m\u001b[38;5;241;40m1\u001b[39;49m\u001b[40m,\u001b[49m\u001b[38;5;241;40m-\u001b[39;49m\u001b[40mADDED_PARAM_LEN\u001b[49m\u001b[40m:\u001b[49m\u001b[40m]\u001b[49m\u001b[38;5;241;40m.\u001b[39;49m\u001b[40mview\u001b[49m\u001b[40m(\u001b[49m\u001b[38;5;241;40m1\u001b[39;49m\u001b[40m,\u001b[49m\u001b[40m \u001b[49m\u001b[38;5;241;40m2\u001b[39;49m\u001b[40m)\u001b[49m\n\u001b[0;32m     49\u001b[0m next_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((next_token, added_params), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# B, 3\u001b[39;00m\n\u001b[0;32m     50\u001b[0m out\u001b[38;5;241m.\u001b[39mappend([token] \u001b[38;5;241m+\u001b[39m added_params\u001b[38;5;241m.\u001b[39mtolist()[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[1, 2]' is invalid for input of size 3"
     ]
    }
   ],
   "source": [
    "for sample in range(1):\n",
    "    gen_ctx = torch.tensor([[[1, 0.0, 0.0, 0.0]] * (CONTEXT_LEN - 1) + [[2, 0.0, 0.0, 0.0]]], device='cuda')\n",
    "    generated = model.generate(gen_ctx, max_new_tokens=1_000)\n",
    "    with open(f\"sample-{sample}.tokens\", 'wb') as f:\n",
    "        cbor2.dump(generated, f)\n",
    "        print(\"wrote\", f.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "6661ae5a-db53-495d-93ca-27f63bfb7249",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train 0.043405480682849884\n",
      "eval 12.46329116821289\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def split_loss(split):\n",
    "    model.eval()\n",
    "    data = {\n",
    "        \"train\": train_data,\n",
    "        \"eval\": eval_data,\n",
    "    }[split]\n",
    "    ix = torch.randint(len(data) - CONTEXT_LEN, (BATCH_SIZE,))\n",
    "    x = torch.stack([data[i:i+CONTEXT_LEN] for i in ix]).cuda()\n",
    "    y = torch.stack([data[i+1:i+CONTEXT_LEN+1] for i in ix]).cuda()\n",
    "    logits, loss = model(x, y)\n",
    "    print(split, loss.item())\n",
    "    model.train()\n",
    "\n",
    "split_loss(\"train\")\n",
    "split_loss(\"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80577faf-3600-40d2-8e5a-1a38293716dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
